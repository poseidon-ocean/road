# 一致性哈希
* 未来订单量比较大，需要多注意系统的可扩展性
* 未来2年内，系统预估的总订单数量可达一亿条
* 按mysql单表存储500万条记录来算，暂时不必分库，单库30个分表是比较合适的水平表方案
	* 1.订单表创建单库30个分表
	* 2.对用户ID和30进行取模，取模结果决定了记录存于第几个分表
	* 3.查询时需要以用户ID作为条件，根据取模结果确定查询哪一个分表
* 分表方式，简化为5个分表
	* userId = 100  100%5=0 -- 分表0
	* userId = 33  33%5=0 -- 分表3
	* userId = 57  57%5=0 -- 分表2
* 上线刚半年多，订单量已经接近一亿了
* 数据量这么大，当初设计的30个分表就不够用了，每个分表数据太多会影响性能的
	* 直接增加分表肯定不行，原先的哈希规则会被打乱
	* 做全量数据迁移倒是能解决问题，但是迁移的代价实在太大了
* 一致性哈希DHT，是麻省理工学院提出的一种算法，目前主要应用于分布式缓存当中
* 一致性哈希可以有效地解决分布式存储结构下动态增加和删除节点所带来的问题
	* 1.首先，我们把全量的缓存空间当做一个环形存储结构。环形空间总共分成2^32个缓存区，在redis中则是把缓存key分配到16384个slot
	* 2.每一个缓存key都可以通过Hash算法转化为一个32位的二进制数，也就对应着环形空间的不同位置
	* 3.我们的每一个缓存节点(shard)也遵循同样的hash算法，比如利用IP做hash，映射到环形空间当中
	* 4.如何让key和节点对应起来呢？ 每一个可以的顺时针方向最近节点，就是key所归属的存储节点
* 当缓存的节点有增加或删除的时候，一致性哈希的优势就显现出来了
	* 1.增加节点
		* 当环信集群的节点有所增加的时候，整个环形空间的映射仍然会保持一致性哈希的顺时针规则，所以有一小部分key的归属会受到影响
		* 有哪些key会受到影响？加入新节点node4，处于node1和node2之间，按照顺时针规则，从node1到node4之间的缓存不再归属于node2
		* 而是归属于新节点node4，因此受影响的key只有key2
		* 最终把key2的缓存数据从node2迁移到node4，就形成了新的符合一致性哈希规则的缓存结构
	* 2.删除节点
		* 当缓存集群的节点需要删除的时候（比如节点挂掉），整个环形空间的映射同样会保持一致性哈希的顺时针规则，同样有一小部分key的归属会受到影响
		* 删除原节点node3，按照顺时针规则，原本node3所拥有的缓存数据就需要托付给node3的顺时针后继节点node1，受影响的只有key4
		* 最终把key4的缓存数据从node3迁移到node1，就形成了新的符合一致性哈希规则的缓存结构
* 节点node3挂掉了，它还能怎么迁移数据到node1节点
	* 这里所说的迁移不是直接数据迁移，而是在查询时去找顺时针的后继节点，因缓存未命中而刷新缓存
* 缓存节点都是按ip来hash的环形空间，如果出现分布不均匀的情况怎么办
	* 为了优化这种节点太少而产生的不均衡情况，一致性哈希算法引入了[虚拟节点]的概念
	* 虚拟节点是基于原来的物理节点映射到N个子节点，最后把所有的子节点映射到环形空间上
	* 假如node1的ip是192.168.1.109，那么原node1节点在环形空间的位置就是hash("192.168.1.109")
	* 基于node1构建两个虚拟节点，node1-1和node1-2，虚拟机节点在环形空间的位置可以利用(IP+后缀)计算
		* hash("192.168.1.109#1")  hash("192.168.1.109#2")
	* 此时环形空间不再用物理节点node1、node2，只有虚拟节点node1-1、node1-2、node2-1、node2-2
	* 由于虚拟节点数量较多，缓存key与虚拟节点的映射关系也变得相对均衡了
* 为什么一致性哈希算法更多应用于像redis这样的缓存数据库
	* 由于分布式缓存系统的节点部署变化更频繁，而传统型关系型数据库的分库分表相对稳定
* 在mysql分库分表的过程中，也可以采用一致性哈希的思想，虽然处理逻辑会复杂一些，却可以避免动态水平扩展时候的尴尬
	

> http://mp.weixin.qq.com/s/yimfkNYF_tIJJqUIzV7TFA
	